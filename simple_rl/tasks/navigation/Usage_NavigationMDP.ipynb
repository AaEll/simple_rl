{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation MDP [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Imports.\n",
    "from simple_rl.tasks import NavigationMDP\n",
    "from simple_rl.agents import QLearningAgent\n",
    "from simple_rl.planning import ValueIteration\n",
    "from simple_rl.tasks.grid_world.GridWorldStateClass import GridWorldState\n",
    "\n",
    "# Python Imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "nvmdp = NavigationMDP(width=30, height=30, \n",
    "                      living_cell_types=['white', 'yellow', 'red', 'lime', 'magenta'],\n",
    "                      living_cell_rewards=[0, 0, -10, -10, -10],\n",
    "                      living_cell_distribution=\"probability\",\n",
    "                      living_cell_type_probs=[0.68, 0.17, 0.05, 0.05, 0.05],\n",
    "                      goal_cell_locs=[(21,21)],\n",
    "                      goal_cell_rewards=[1.],\n",
    "                      goal_cell_types=[\"blue\"],\n",
    "                      slip_prob=0.00, step_cost=0.0, gamma=.99)\n",
    "# Use \"init_states\" to request specific init states while sampling trajectories\n",
    "# if \"n_trajectory\" is greater than # of init_states, remaining init states will be sampled randomly\n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=8, \n",
    "                                                            init_states=[GridWorldState(2,2)], \n",
    "                                                            init_repetition=False)\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features used for short horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample State Trajectory \\nFeatures: Cell Indicator {} + Distsances to {} + Distance to {}\\n\".format(nvmdp.cell_types, nvmdp.cell_types, nvmdp.goal_cell_types))\n",
    "[nvmdp.feature_at_state(s,\n",
    "                        feature_type=\"indicator\",\n",
    "                        incl_cell_distances=True,\n",
    "                        incl_goal_indicator=False,\n",
    "                        incl_goal_distances=True,\n",
    "                        normalize_distance=False, dtype=np.float).tolist() for s in state_trajectories[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features used for long horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample State Trajectory \\nFeatures: Cell Indicator {} + Distance to {}\\n\".format(nvmdp.cell_types, nvmdp.goal_cell_types))\n",
    "[nvmdp.feature_at_state(s,\n",
    "                        feature_type=\"indicator\",\n",
    "                        incl_cell_distances=False,\n",
    "                        incl_goal_indicator=False,\n",
    "                        incl_goal_distances=True,\n",
    "                        normalize_distance=False, dtype=np.float).tolist() for s in state_trajectories[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "nvmdp = NavigationMDP(width=30, height=30, \n",
    "                      living_cell_types=['white', 'yellow', 'red', 'lime', 'magenta'],\n",
    "                      living_cell_rewards=[0, 0, -10, -10, -10],\n",
    "                      living_cell_distribution=\"probability\",\n",
    "                      living_cell_type_probs=[0.68, 0.17, 0.05, 0.05, 0.05],\n",
    "                      goal_cell_locs=[(21,21), (11,11)],\n",
    "                      goal_cell_rewards=[1., 1.5],\n",
    "                      goal_cell_types=[\"orange\",\"blue\"],\n",
    "                      slip_prob=0.00, step_cost=0.0, gamma=.95)\n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=8, \n",
    "                                                            init_states=[GridWorldState(2,2)], \n",
    "                                                            init_repetition=False)\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\")\n",
    "## Features: Cell Type Ind + Cell Type Dist + Goal Dist\n",
    "print(\"Sample State Trajectory\")\n",
    "[nvmdp.feature_at_state(s,\n",
    "                        feature_type=\"indicator\",\n",
    "                        incl_cell_distances=False,\n",
    "                        incl_goal_indicator=False,\n",
    "                        incl_goal_distances=True,\n",
    "                        normalize_distance=False, dtype=np.float).tolist() for s in state_trajectories[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp = NavigationMDP(width=7, height=7,\n",
    "                      living_cell_types=['white', 'yellow', 'red'],\n",
    "                      living_cell_rewards=[0, 0, -10],\n",
    "                      living_cell_distribution=\"manual\",\n",
    "                      living_cell_locs=[np.inf, np.inf, [(2,i) for i in range(1,7)]],\n",
    "                      goal_cell_locs=[(7,1),(1,1)],\n",
    "                      goal_cell_types=[\"blue\", \"orange\"],\n",
    "                      goal_cell_rewards=[1.,10.],\n",
    "                      traj_init_cell_types=[0,1],\n",
    "                      slip_prob=0.00, step_cost=0.0, gamma=.50)\n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=41, init_states=[GridWorldState(1,1)], init_repetition=False)\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Goals dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp._reset_goals([(7,1),(1,1)],[10.,1.],[\"blue\", \"orange\"])\n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=41, init_states=[GridWorldState(1,1)], init_repetition=False)\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp._reset_goals([(7,1),(1,1),(7,7)],[10.,1.,10.],[\"blue\", \"orange\",\"purple\"])\n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=41, init_states=[GridWorldState(1,1)], init_repetition=False)\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp._reset_goals([(7,1),(1,1),(7,7),(4,4)],[10.,1.,10.,5.],list(range(4))) \n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=41, init_states=[GridWorldState(1,1)], init_repetition=False)\n",
    "plt.figure(figsize=(14,8))\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\", new_fig=False, subplot_str=\"121\")\n",
    "nvmdp.visualize_grid(nvmdp.get_value_grid(), trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=False, goal_marker=\"*c\", new_fig=False, subplot_str=\"122\", \n",
    "                     state_space_cmap=False, title=\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Rewards dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp.living_cell_rewards, nvmdp.goal_cell_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp._reset_rewards([0, 0, -0.01], [10.0, 1.0, 10.0, 5.0])\n",
    "state_trajectories, action_trajectories = nvmdp.sample_data(n_trajectory=41, init_states=[GridWorldState(1,1)], init_repetition=False)\n",
    "plt.figure(figsize=(14,8))\n",
    "nvmdp.visualize_grid(trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=True, goal_marker=\"*c\", new_fig=False, subplot_str=\"121\")\n",
    "nvmdp.visualize_grid(nvmdp.get_value_grid(), trajectories=state_trajectories, show_colorbar=True, show_rewards_colorbar=False, goal_marker=\"*c\", new_fig=False, subplot_str=\"122\", \n",
    "                     state_space_cmap=False, title=\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] MacGlashan, James, and Michael L. Littman. \"Between Imitation and Intention Learning.\" IJCAI. 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl_python3",
   "language": "python",
   "name": "irl_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
